# ğŸ¦¥ Unslothå®Œå…¨ã‚¬ã‚¤ãƒ‰

Unslothã¯ã€LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€Unslothã®ä½¿ã„æ–¹ã‚’è©³ã—ãèª¬æ˜ã—ã¾ã™ã€‚

## ğŸ“‹ ç›®æ¬¡

1. [Unslothã¨ã¯](#unslothã¨ã¯)
2. [Web UIã§ã®ä½¿ç”¨](#web-uiã§ã®ä½¿ç”¨)
3. [ã‚³ãƒ¼ãƒ‰ã§ã®ä½¿ç”¨](#ã‚³ãƒ¼ãƒ‰ã§ã®ä½¿ç”¨)
4. [æ¨å¥¨è¨­å®š](#æ¨å¥¨è¨­å®š)
5. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)

## Unslothã¨ã¯

### ç‰¹å¾´

- ğŸš€ **2å€é«˜é€Ÿãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°** - å¾“æ¥ã®æ–¹æ³•ã¨æ¯”è¼ƒã—ã¦2å€é«˜é€Ÿ
- ğŸ’¾ **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡** - 4bité‡å­åŒ–ã«ã‚ˆã‚Šå°‘ãªã„VRAMã§å¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’æ‰±ãˆã‚‹
- âš¡ **é«˜é€Ÿæ¨è«–** - æœ€é©åŒ–ã•ã‚ŒãŸæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³
- ğŸ”§ **ç°¡å˜ãªçµ±åˆ** - LLaMA-Factoryã¨å®Œå…¨çµ±åˆ

### å¯¾å¿œãƒ¢ãƒ‡ãƒ«

Unslothã¯ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ï¼š
- Llama (3, 3.1, 3.2)
- Mistral
- Qwen
- Phi
- Gemma
- ãã®ä»–å¤šæ•°

## Web UIã§ã®ä½¿ç”¨

### åŸºæœ¬çš„ãªä½¿ã„æ–¹

#### 1. Web UIã‚’èµ·å‹•

```bash
cd LLaMA-Factory
source .venv/bin/activate
llamafactory-cli webui
```

ãƒ–ãƒ©ã‚¦ã‚¶ã§ `http://127.0.0.1:7860` ãŒé–‹ãã¾ã™ã€‚

#### 2. Boosterã§ã€Œunslothã€ã‚’é¸æŠ

Web UIã®ä¸Šéƒ¨ã«ã‚ã‚‹ **Booster** ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã§ `unsloth` ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

```
Booster: [unsloth â–¼]
```

**ã“ã‚Œã ã‘ã§UnslothãŒæœ‰åŠ¹ã«ãªã‚Šã¾ã™ï¼**

### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨­å®šï¼ˆTrainã‚¿ãƒ–ï¼‰

#### æ¨å¥¨è¨­å®šï¼ˆRTX 4060 Ti 8GBï¼‰

**åŸºæœ¬è¨­å®š:**
- **Model name**: `unsloth/llama-3.2-3b-bnb-4bit`
- **Finetuning type**: `lora`
- **Quantization bit**: `4`
- **Quantization method**: `bnb`
- **Booster**: `unsloth` â† **é‡è¦ï¼**

**LoRAè¨­å®š:**
- **LoRA rank**: `32`
- **LoRA alpha**: `32`
- **LoRA target**: `all`

**ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**
- **Learning rate**: `2e-4`
- **Epochs**: `3`
- **Batch size**: `2`
- **Gradient accumulation**: `8`
- **Max length**: `2048`

### ãƒãƒ£ãƒƒãƒˆè¨­å®šï¼ˆChatã‚¿ãƒ–ï¼‰

**åŸºæœ¬è¨­å®š:**
- **Model name**: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
- **Finetuning type**: `lora`
- **Checkpoint path**: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
- **Booster**: `unsloth` â† **é‡è¦ï¼**

**ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:**
- **Temperature**: `0.7`
- **Top-p**: `0.9`
- **Max new tokens**: `512`

## ã‚³ãƒ¼ãƒ‰ã§ã®ä½¿ç”¨

### æ¨è«–ã®ä¾‹

```python
from unsloth import FastLanguageModel

# ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3.2-3b-bnb-4bit",
    max_seq_length=2048,
    dtype=None,  # è‡ªå‹•æ¤œå‡º
    load_in_4bit=True,
)

# æ¨è«–ç”¨ã«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
FastLanguageModel.for_inference(model)

# ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
inputs = tokenizer(
    ["ã“ã‚“ã«ã¡ã¯ã€ä»Šæ—¥ã®å¤©æ°—ã¯ï¼Ÿ"],
    return_tensors="pt"
).to("cuda")

outputs = model.generate(
    **inputs,
    max_new_tokens=128,
    temperature=0.7,
    top_p=0.9,
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

### ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ä¾‹

```python
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments

# ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/llama-3.2-3b-bnb-4bit",
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)

# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®è¿½åŠ 
model = FastLanguageModel.get_peft_model(
    model,
    r=32,  # LoRAãƒ©ãƒ³ã‚¯
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    use_gradient_checkpointing="unsloth",  # Unslothæœ€é©åŒ–
)

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=2048,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=8,
        learning_rate=2e-4,
        num_train_epochs=3,
        fp16=True,
        optim="adamw_8bit",
        output_dir="outputs",
    ),
)

trainer.train()
```

## æ¨å¥¨è¨­å®š

### RTX 4060 Ti 8GBå‘ã‘

#### å°å‹ãƒ¢ãƒ‡ãƒ«ï¼ˆ1B-3Bï¼‰

```yaml
model_name_or_path: unsloth/llama-3.2-3b-bnb-4bit
quantization_bit: 4
use_unsloth: true

per_device_train_batch_size: 2
gradient_accumulation_steps: 8
cutoff_len: 2048
```

#### è¶…è»½é‡ãƒ¢ãƒ‡ãƒ«ï¼ˆ1Bï¼‰

```yaml
model_name_or_path: unsloth/llama-3.2-1b-bnb-4bit
quantization_bit: 4
use_unsloth: true

per_device_train_batch_size: 4
gradient_accumulation_steps: 4
cutoff_len: 2048
```

### ã‚ˆã‚Šå¤§ãã„VRAMï¼ˆ16GB+ï¼‰å‘ã‘

```yaml
model_name_or_path: unsloth/Meta-Llama-3.1-8B-bnb-4bit
quantization_bit: 4
use_unsloth: true

per_device_train_batch_size: 2
gradient_accumulation_steps: 8
cutoff_len: 4096
```

## Unslothæœ€é©åŒ–ãƒ¢ãƒ‡ãƒ«

ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã¯Unslothã§æœ€é©åŒ–ã•ã‚Œã¦ãŠã‚Šã€ã™ãã«ä½¿ç”¨ã§ãã¾ã™ï¼š

### å°å‹ãƒ¢ãƒ‡ãƒ«ï¼ˆ8GB VRAMï¼‰
- `unsloth/llama-3.2-1b-bnb-4bit`
- `unsloth/llama-3.2-3b-bnb-4bit`
- `unsloth/Phi-3.5-mini-instruct-bnb-4bit`

### ä¸­å‹ãƒ¢ãƒ‡ãƒ«ï¼ˆ16GB VRAMæ¨å¥¨ï¼‰
- `unsloth/Meta-Llama-3.1-8B-bnb-4bit`
- `unsloth/mistral-7b-v0.3-bnb-4bit`
- `unsloth/Qwen2.5-7B-bnb-4bit`

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### CUDA Out of Memory

**è§£æ±ºç­–:**
1. ã‚ˆã‚Šå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆ3B â†’ 1Bï¼‰
2. `per_device_train_batch_size` ã‚’æ¸›ã‚‰ã™ï¼ˆ2 â†’ 1ï¼‰
3. `cutoff_len` ã‚’æ¸›ã‚‰ã™ï¼ˆ2048 â†’ 1024ï¼‰
4. `gradient_accumulation_steps` ã‚’å¢—ã‚„ã™

### UnslothãŒé¸æŠã§ããªã„

**ç¢ºèªäº‹é …:**
```bash
# Unslothã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
python -c "import unsloth; print(unsloth.__version__)"

# ç’°å¢ƒãŒæ­£ã—ãã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‹
which python
```

### ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼

**è§£æ±ºç­–:**
```bash
# ç’°å¢ƒã‚’å†åŒæœŸ
cd LLaMA-Factory
uv sync
```

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒé…ã„

**ç¢ºèªäº‹é …:**
1. Boosterã§ `unsloth` ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹ã‹
2. `use_unsloth: true` ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ï¼ˆYAMLï¼‰
3. GPUãŒæ­£ã—ãèªè­˜ã•ã‚Œã¦ã„ã‚‹ã‹
   ```bash
   nvidia-smi
   ```

## ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ï¼ˆRTX 4060 Ti 8GBï¼‰

| ãƒ¢ãƒ‡ãƒ« | é€šå¸¸ | Unsloth | é«˜é€ŸåŒ–ç‡ |
|--------|------|---------|----------|
| Llama 3.2 1B | 100% | 210% | 2.1x |
| Llama 3.2 3B | 100% | 195% | 1.95x |
| Phi-3.5 Mini | 100% | 200% | 2.0x |

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡

| ãƒ¢ãƒ‡ãƒ« | é€šå¸¸ï¼ˆ16bitï¼‰ | 4bité‡å­åŒ– | å‰Šæ¸›ç‡ |
|--------|---------------|------------|--------|
| Llama 3.2 3B | ~12GB | ~3GB | 75% |
| Llama 3.2 1B | ~4GB | ~1GB | 75% |

## å‚è€ƒãƒªãƒ³ã‚¯

- [Unsloth GitHub](https://github.com/unslothai/unsloth)
- [Unsloth ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.unsloth.ai/)
- [å¯¾å¿œãƒ¢ãƒ‡ãƒ«ä¸€è¦§](https://huggingface.co/unsloth)

---

**Unslothã§é«˜é€Ÿãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ¥½ã—ã‚“ã§ãã ã•ã„ï¼** ğŸ¦¥âš¡
