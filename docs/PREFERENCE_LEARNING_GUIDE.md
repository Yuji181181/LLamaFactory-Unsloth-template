# LLaMA-Factory é¸å¥½å­¦ç¿’æ‰‹æ³•ã‚¬ã‚¤ãƒ‰

LLaMA-Factoryã¯ã€DPOä»¥å¤–ã«ã‚‚è¤‡æ•°ã®é¸å¥½å­¦ç¿’ï¼ˆPreference Learningï¼‰æ‰‹æ³•ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚

## ğŸ“Š ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹æ‰‹æ³•ä¸€è¦§

LLaMA-Factoryã§ã¯ã€ä»¥ä¸‹ã®6ã¤ã®é¸å¥½å­¦ç¿’æ‰‹æ³•ãŒä½¿ç”¨ã§ãã¾ã™ï¼ˆ`pref_loss`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æŒ‡å®šï¼‰ï¼š

### 1. **sigmoid** (DPO - Direct Preference Optimization)
- **ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ‰‹æ³•**
- æœ€ã‚‚ä¸€èˆ¬çš„ã§å®‰å®šã—ãŸæ‰‹æ³•
- å‚ç…§ãƒ¢ãƒ‡ãƒ«ï¼ˆreference modelï¼‰ãŒå¿…è¦

**ç‰¹å¾´ï¼š**
- âœ… å®‰å®šã—ãŸå­¦ç¿’
- âœ… åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹
- âœ… è«–æ–‡ã§æ¤œè¨¼æ¸ˆã¿
- âŒ å‚ç…§ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„ï¼‰

**æ¨å¥¨è¨­å®šï¼š**
```yaml
stage: dpo
pref_loss: sigmoid
pref_beta: 0.1  # 0.1-0.5ãŒä¸€èˆ¬çš„
pref_ftx: 0.0   # SFTæå¤±ã®é‡ã¿
```

### 2. **hinge** (Hinge Loss DPO)
- Hingeæå¤±ã‚’ä½¿ç”¨ã—ãŸDPOã®å¤‰ç¨®
- ãƒãƒ¼ã‚¸ãƒ³ãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’

**ç‰¹å¾´ï¼š**
- âœ… ãƒãƒ¼ã‚¸ãƒ³ã‚’æ˜ç¤ºçš„ã«æœ€å¤§åŒ–
- âœ… ãƒ­ãƒã‚¹ãƒˆãªå­¦ç¿’
- âŒ å‚ç…§ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦

**æ¨å¥¨è¨­å®šï¼š**
```yaml
stage: dpo
pref_loss: hinge
pref_beta: 0.1
```

### 3. **ipo** (Identity Preference Optimization)
- DPOã®æ”¹è‰¯ç‰ˆ
- ã‚ˆã‚Šå®‰å®šã—ãŸå­¦ç¿’ãŒå¯èƒ½

**ç‰¹å¾´ï¼š**
- âœ… DPOã‚ˆã‚Šå®‰å®š
- âœ… éå­¦ç¿’ã—ã«ãã„
- âŒ å‚ç…§ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦

**æ¨å¥¨è¨­å®šï¼š**
```yaml
stage: dpo
pref_loss: ipo
pref_beta: 0.1
```

### 4. **kto_pair** (KTO - Kahneman-Tversky Optimization)
- ãƒšã‚¢ãƒ¯ã‚¤ã‚ºãƒ‡ãƒ¼ã‚¿ç”¨ã®KTO
- äººé–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’ã‚ˆã‚ŠåŠ¹æœçš„ã«æ´»ç”¨

**ç‰¹å¾´ï¼š**
- âœ… å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§åŠ¹æœçš„
- âœ… äººé–“ã®åˆ¤æ–­ã«ã‚ˆã‚Šè¿‘ã„
- âŒ å‚ç…§ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦

**æ¨å¥¨è¨­å®šï¼š**
```yaml
stage: dpo  # ã¾ãŸã¯ kto
pref_loss: kto_pair
pref_beta: 0.1
kto_chosen_weight: 1.0
kto_rejected_weight: 1.0
```

### 5. **orpo** (ORPO - Odds Ratio Preference Optimization) â­
- **å‚ç…§ãƒ¢ãƒ‡ãƒ«ä¸è¦**
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒè‰¯ã„
- SFTã¨ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’åŒæ™‚ã«å®Ÿè¡Œ

**ç‰¹å¾´ï¼š**
- âœ… å‚ç…§ãƒ¢ãƒ‡ãƒ«ä¸è¦ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
- âœ… SFTã¨ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’çµ±åˆ
- âœ… é«˜é€Ÿ
- âœ… ã‚·ãƒ³ãƒ—ãƒ«

**æ¨å¥¨è¨­å®šï¼š**
```yaml
stage: dpo
pref_loss: orpo
pref_beta: 0.1
```

### 6. **simpo** (SimPO - Simple Preference Optimization) â­â­
- **å‚ç…§ãƒ¢ãƒ‡ãƒ«ä¸è¦**
- æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã§åŠ¹ç‡çš„
- æœ€æ–°ã®æ‰‹æ³•

**ç‰¹å¾´ï¼š**
- âœ… å‚ç…§ãƒ¢ãƒ‡ãƒ«ä¸è¦ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰
- âœ… éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«
- âœ… é«˜æ€§èƒ½
- âœ… å®Ÿè£…ãŒè»½é‡
- âœ… **æ¨å¥¨ï¼**

**æ¨å¥¨è¨­å®šï¼š**
```yaml
stage: dpo
pref_loss: simpo
pref_beta: 2.0  # SimPOã¯å¤§ãã‚ã®betaã‚’ä½¿ç”¨
simpo_gamma: 0.5  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒªãƒ¯ãƒ¼ãƒ‰ãƒãƒ¼ã‚¸ãƒ³
```

## ğŸ¯ ã©ã®æ‰‹æ³•ã‚’é¸ã¶ã¹ãã‹ï¼Ÿ

### ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã§ã®æ¨å¥¨é †ä½

#### 1ä½: **SimPO** ğŸ¥‡
```yaml
pref_loss: simpo
pref_beta: 2.0
simpo_gamma: 0.5
```
**ç†ç”±ï¼š**
- å‚ç…§ãƒ¢ãƒ‡ãƒ«ä¸è¦ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒè‰¯ã„
- æœ€æ–°ã®æ‰‹æ³•ã§é«˜æ€§èƒ½
- å®Ÿè£…ãŒã‚·ãƒ³ãƒ—ãƒ«ã§å®‰å®š
- **8GB VRAMã§ã‚‚ä½¿ã„ã‚„ã™ã„**

#### 2ä½: **ORPO** ğŸ¥ˆ
```yaml
pref_loss: orpo
pref_beta: 0.1
```
**ç†ç”±ï¼š**
- å‚ç…§ãƒ¢ãƒ‡ãƒ«ä¸è¦
- SFTã¨ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’çµ±åˆ
- å®‰å®šã—ãŸæ€§èƒ½

#### 3ä½: **DPO (sigmoid)** ğŸ¥‰
```yaml
pref_loss: sigmoid
pref_beta: 0.1
```
**ç†ç”±ï¼š**
- æœ€ã‚‚ä¸€èˆ¬çš„ã§æ¤œè¨¼æ¸ˆã¿
- å®‰å®šã—ãŸå­¦ç¿’
- ãŸã ã—å‚ç…§ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ï¼ˆãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„ï¼‰

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¯”è¼ƒ

| æ‰‹æ³• | å‚ç…§ãƒ¢ãƒ‡ãƒ« | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | æ¨å¥¨VRAM |
|------|-----------|-------------|----------|
| SimPO | ä¸è¦ | ä½ | 8GB+ |
| ORPO | ä¸è¦ | ä½ | 8GB+ |
| DPO (sigmoid) | å¿…è¦ | é«˜ | 16GB+ |
| IPO | å¿…è¦ | é«˜ | 16GB+ |
| Hinge | å¿…è¦ | é«˜ | 16GB+ |
| KTO | å¿…è¦ | é«˜ | 16GB+ |

## ğŸ“ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

### SimPOè¨­å®šï¼ˆæ¨å¥¨ï¼‰

```yaml
# configs/simpo_config.yaml
### ãƒ¢ãƒ‡ãƒ«è¨­å®š
model_name_or_path: unsloth/llama-3.2-3b-bnb-4bit
adapter_name_or_path: outputs/sft_model/checkpoint-best

stage: dpo
do_train: true
finetuning_type: lora

### LoRA ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
lora_target: all
lora_rank: 32
lora_alpha: 32
lora_dropout: 0.05

### é‡å­åŒ–è¨­å®š
quantization_bit: 4
quantization_method: bnb

### Unslothæœ€é©åŒ–
use_unsloth: true
use_unsloth_gc: true

### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
dataset: competition_dpo
template: llama3
cutoff_len: 2048
val_size: 0.1
overwrite_cache: true

### SimPOå›ºæœ‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
pref_loss: simpo  # SimPOã‚’ä½¿ç”¨
pref_beta: 2.0    # SimPOã¯å¤§ãã‚ã®beta
simpo_gamma: 0.5  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒªãƒ¯ãƒ¼ãƒ‰ãƒãƒ¼ã‚¸ãƒ³
pref_ftx: 0.0     # SFTæå¤±ã®é‡ã¿

### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
output_dir: outputs/simpo_model
overwrite_output_dir: true

per_device_train_batch_size: 2  # SimPOã¯å‚ç…§ãƒ¢ãƒ‡ãƒ«ä¸è¦ãªã®ã§å¤§ãã‚ã§OK
gradient_accumulation_steps: 8
per_device_eval_batch_size: 2

learning_rate: 5.0e-6
lr_scheduler_type: cosine
warmup_ratio: 0.1
num_train_epochs: 1

### æœ€é©åŒ–è¨­å®š
optim: adamw_torch
fp16: true
max_grad_norm: 1.0

### ãƒ­ã‚®ãƒ³ã‚°ã¨ä¿å­˜
logging_steps: 5
save_steps: 50
save_total_limit: 3
eval_strategy: steps
eval_steps: 50

### ãã®ä»–
seed: 42
report_to: none
dataloader_num_workers: 4
remove_unused_columns: false
save_safetensors: true
```

### ORPOè¨­å®š

```yaml
# configs/orpo_config.yaml
# ï¼ˆSimPOã¨ã»ã¼åŒã˜ã ãŒã€ä»¥ä¸‹ã‚’å¤‰æ›´ï¼‰

pref_loss: orpo
pref_beta: 0.1  # ORPOã¯å°ã•ã‚ã®beta
# simpo_gamma ã¯ä¸è¦
```

### DPOè¨­å®šï¼ˆå¾“æ¥å‹ï¼‰

```yaml
# configs/dpo_config.yaml
# ï¼ˆå‚ç…§ãƒ¢ãƒ‡ãƒ«ãŒå¿…è¦ï¼‰

pref_loss: sigmoid
pref_beta: 0.1

# å‚ç…§ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€æŒ‡å®šã—ãªã„å ´åˆã¯ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
# ref_model: unsloth/llama-3.2-3b-bnb-4bit
# ref_model_quantization_bit: 4

# ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ã
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
```

## ğŸ”¬ å®Ÿé¨“ã®æ¨å¥¨é †åº

### ã‚¹ãƒ†ãƒƒãƒ—1: SFT
ã¾ãšã€SFTã§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ

### ã‚¹ãƒ†ãƒƒãƒ—2: SimPOã§è©¦ã™
```bash
llamafactory-cli train configs/simpo_config.yaml
```

### ã‚¹ãƒ†ãƒƒãƒ—3: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´
- `pref_beta`: 1.0, 2.0, 3.0
- `simpo_gamma`: 0.3, 0.5, 0.7
- `learning_rate`: 1e-6, 5e-6, 1e-5

### ã‚¹ãƒ†ãƒƒãƒ—4: ä»–ã®æ‰‹æ³•ã‚‚è©¦ã™ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- ORPO
- DPO (ãƒ¡ãƒ¢ãƒªã«ä½™è£•ãŒã‚ã‚Œã°)

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

ã™ã¹ã¦ã®é¸å¥½å­¦ç¿’æ‰‹æ³•ã§ã€åŒã˜ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä½¿ç”¨ã§ãã¾ã™ï¼š

```json
[
  {
    "conversations": [
      {"from": "human", "value": "è³ªå•"},
      {"from": "gpt", "value": "å¥½ã¾ã—ã„å›ç­”"}
    ],
    "rejected_conversations": [
      {"from": "human", "value": "è³ªå•"},
      {"from": "gpt", "value": "å¥½ã¾ã—ããªã„å›ç­”"}
    ]
  }
]
```

## ğŸ’¡ ã‚³ãƒ³ãƒšã§ã®æˆ¦ç•¥

### RTX 4060 Ti 8GB ã®å ´åˆ

1. **SFT**: `unsloth/llama-3.2-3b-bnb-4bit`
2. **SimPO**: å‚ç…§ãƒ¢ãƒ‡ãƒ«ä¸è¦ãªã®ã§8GBã§å¿«é©ã«å‹•ä½œ
3. **ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´**: beta, gamma, learning rateã‚’èª¿æ•´

### ã‚ˆã‚Šå¤§ãã„VRAMï¼ˆ16GB+ï¼‰ã®å ´åˆ

1. **SFT**: `unsloth/Meta-Llama-3.1-8B-bnb-4bit`
2. **SimPO, ORPO, DPO**ã‚’å…¨ã¦è©¦ã™
3. **ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«**: è¤‡æ•°ã®æ‰‹æ³•ã®çµæœã‚’çµ„ã¿åˆã‚ã›ã‚‹

## ğŸ“š å‚è€ƒè«–æ–‡

- **DPO**: [Direct Preference Optimization](https://arxiv.org/abs/2305.18290)
- **IPO**: [A General Theoretical Paradigm to Understand Learning from Human Preferences](https://arxiv.org/abs/2310.12036)
- **KTO**: [KTO: Model Alignment as Prospect Theoretic Optimization](https://arxiv.org/abs/2402.01306)
- **ORPO**: [ORPO: Monolithic Preference Optimization without Reference Model](https://arxiv.org/abs/2403.07691)
- **SimPO**: [SimPO: Simple Preference Optimization with a Reference-Free Reward](https://arxiv.org/abs/2405.14734)

## ğŸš€ ã¾ã¨ã‚

**ã‚³ãƒ³ãƒšã§æœ€åˆã«è©¦ã™ã¹ãæ‰‹æ³•ï¼š**
1. **SimPO** - å‚ç…§ãƒ¢ãƒ‡ãƒ«ä¸è¦ã€é«˜æ€§èƒ½ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡â—
2. **ORPO** - SFTã¨ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã‚’çµ±åˆã€ã‚·ãƒ³ãƒ—ãƒ«
3. **DPO** - å¾“æ¥å‹ã€å®‰å®šã—ã¦ã„ã‚‹ãŒãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤šã„

**RTX 4060 Ti 8GBã§ã¯ SimPO ãŒæœ€é©ï¼** ğŸ¯
