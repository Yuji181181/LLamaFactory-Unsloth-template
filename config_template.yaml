# LLMコンペ用トレーニング設定テンプレート
# LLaMA-Factory + Unsloth

### モデル設定
model_name_or_path: unsloth/llama-3.2-3b-bnb-4bit  # 使用するモデル
# その他の推奨モデル:
# - unsloth/llama-3.2-1b-bnb-4bit (最軽量)
# - unsloth/Phi-3.5-mini-instruct-bnb-4bit (高性能)
# - unsloth/Meta-Llama-3.1-8B-bnb-4bit (大規模、16GB VRAM推奨)

### ファインチューニング設定
stage: sft  # Supervised Fine-Tuning
do_train: true
finetuning_type: lora  # LoRA（低ランク適応）

### LoRA パラメータ
lora_target: all  # すべてのターゲットモジュール
# または個別指定: q_proj,v_proj,k_proj,o_proj,gate_proj,up_proj,down_proj
lora_rank: 32  # LoRAランク（8, 16, 32, 64）
lora_alpha: 32  # LoRA alpha（通常はrankと同じ）
lora_dropout: 0.05  # ドロップアウト率

### 量子化設定
quantization_bit: 4  # 4bit量子化
quantization_method: bnb  # bitsandbytes

### Unsloth最適化（重要！）
use_unsloth: true  # Unslothを有効化
use_unsloth_gc: true  # Unsloth gradient checkpointing

### データセット設定
dataset: your_dataset_name  # data/ ディレクトリ内のデータセット名
template: llama3  # プロンプトテンプレート（llama3, alpaca, など）
cutoff_len: 2048  # 最大シーケンス長
# メモリ不足の場合は 1024 に減らす

### データセット分割
val_size: 0.1  # 検証データの割合（10%）
overwrite_cache: true  # キャッシュを上書き

### トレーニングパラメータ
output_dir: outputs/experiment_001  # 出力ディレクトリ
overwrite_output_dir: true

# バッチサイズとアキュムレーション
per_device_train_batch_size: 2  # GPUあたりのバッチサイズ
# メモリ不足の場合は 1 に減らす
gradient_accumulation_steps: 8  # 勾配累積ステップ
# 実効バッチサイズ = per_device_train_batch_size * gradient_accumulation_steps

# 学習率
learning_rate: 2.0e-4  # 学習率（2e-4 または 5e-5）
lr_scheduler_type: cosine  # 学習率スケジューラ（cosine, linear, constant）
warmup_ratio: 0.1  # ウォームアップ比率

# エポック数
num_train_epochs: 3  # トレーニングエポック数
# または max_steps を使用
# max_steps: 1000

### 最適化設定
optim: adamw_torch  # オプティマイザ
# メモリ節約: adamw_8bit

# 混合精度
fp16: true  # FP16（Ampere以前のGPU）
# bf16: true  # BF16（Ampere以降のGPU、RTX 30xx/40xx）

# 勾配クリッピング
max_grad_norm: 1.0

### ロギングと保存
logging_steps: 10  # ログ出力間隔
save_steps: 100  # モデル保存間隔
save_total_limit: 3  # 保存する最大チェックポイント数

# 評価
eval_strategy: steps  # 評価戦略（steps, epoch, no）
eval_steps: 100  # 評価間隔
per_device_eval_batch_size: 2  # 評価時のバッチサイズ

### その他の設定
seed: 42  # 乱数シード（再現性のため）
report_to: none  # レポート先（none, tensorboard, wandb）
# wandbを使用する場合:
# report_to: wandb
# run_name: experiment_001

# データローダー
dataloader_num_workers: 4  # データローダーのワーカー数
remove_unused_columns: false

# 保存設定
save_safetensors: true  # SafeTensors形式で保存
