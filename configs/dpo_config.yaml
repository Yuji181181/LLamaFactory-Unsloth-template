### DPO (Direct Preference Optimization) 設定ファイル
# Unsloth + LLaMA-Factory

### モデル設定
# SFTモデルのチェックポイントを指定
model_name_or_path: unsloth/llama-3.2-3b-bnb-4bit
adapter_name_or_path: outputs/sft_model/checkpoint-best  # SFTで学習したLoRAアダプター

stage: dpo
do_train: true
finetuning_type: lora

### LoRA パラメータ
lora_target: all
lora_rank: 32
lora_alpha: 32
lora_dropout: 0.05

### 量子化設定
quantization_bit: 4
quantization_method: bnb

### Unsloth最適化
use_unsloth: true
use_unsloth_gc: true

### データセット設定
dataset: competition_dpo  # LLaMA-Factory/data/competition_dpo/
template: llama3
cutoff_len: 2048
val_size: 0.1
overwrite_cache: true

### DPO固有パラメータ
pref_beta: 0.1  # DPOのbetaパラメータ（0.1-0.5が一般的）
pref_ftx: 0.0   # SFT損失の重み（0=DPOのみ、1=SFTも含む）
pref_loss: sigmoid  # 損失関数（sigmoid, hinge, ipo, kto_pair）

### トレーニングパラメータ
output_dir: outputs/dpo_model
overwrite_output_dir: true

# バッチサイズ（DPOはメモリを多く使うため、SFTより小さめに）
per_device_train_batch_size: 1
gradient_accumulation_steps: 16
per_device_eval_batch_size: 1

# 学習率（DPOはSFTより小さめが推奨）
learning_rate: 5.0e-6
lr_scheduler_type: cosine
warmup_ratio: 0.1

# エポック数（DPOは少なめでOK）
num_train_epochs: 1

### 最適化設定
optim: adamw_torch
fp16: true
max_grad_norm: 1.0

### ロギングと保存
logging_steps: 5
save_steps: 50
save_total_limit: 3

# 評価
eval_strategy: steps
eval_steps: 50

### その他
seed: 42
report_to: none
dataloader_num_workers: 4
remove_unused_columns: false
save_safetensors: true
