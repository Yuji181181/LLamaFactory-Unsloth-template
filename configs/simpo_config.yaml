### SimPO (Simple Preference Optimization) 設定ファイル
# 参照モデル不要で高性能！コンペ推奨手法
# Unsloth + LLaMA-Factory

### モデル設定
# SFTモデルのチェックポイントを指定
model_name_or_path: unsloth/llama-3.2-3b-bnb-4bit
adapter_name_or_path: outputs/sft_model/checkpoint-best  # SFTで学習したLoRAアダプター

stage: dpo
do_train: true
finetuning_type: lora

### LoRA パラメータ
lora_target: all
lora_rank: 32
lora_alpha: 32
lora_dropout: 0.05

### 量子化設定
quantization_bit: 4
quantization_method: bnb

### Unsloth最適化
use_unsloth: true
use_unsloth_gc: true

### データセット設定
dataset: competition_dpo  # LLaMA-Factory/data/competition_dpo/
template: llama3
cutoff_len: 2048
val_size: 0.1
overwrite_cache: true

### SimPO固有パラメータ
pref_loss: simpo  # SimPOを使用（参照モデル不要！）
pref_beta: 2.0    # SimPOは大きめのbeta（1.0-3.0を試す）
simpo_gamma: 0.5  # ターゲットリワードマージン（0.3-0.7を試す）
pref_ftx: 0.0     # SFT損失の重み（0=SimPOのみ）

### トレーニングパラメータ
output_dir: outputs/simpo_model
overwrite_output_dir: true

# バッチサイズ（SimPOは参照モデル不要なのでDPOより大きめでOK）
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
per_device_eval_batch_size: 2

# 学習率（選好学習はSFTより小さめ）
learning_rate: 5.0e-6  # 1e-6, 5e-6, 1e-5を試す
lr_scheduler_type: cosine
warmup_ratio: 0.1

# エポック数（選好学習は少なめでOK）
num_train_epochs: 1

### 最適化設定
optim: adamw_torch
fp16: true
max_grad_norm: 1.0

### ロギングと保存
logging_steps: 5
save_steps: 50
save_total_limit: 3

# 評価
eval_strategy: steps
eval_steps: 50

### その他
seed: 42
report_to: none
dataloader_num_workers: 4
remove_unused_columns: false
save_safetensors: true
