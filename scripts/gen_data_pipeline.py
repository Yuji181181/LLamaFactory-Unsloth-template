import os
import json
import logging
from typing import List, Dict, Optional
from openai import OpenAI
from tqdm import tqdm
from dotenv import load_dotenv

# .envã®èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# ==========================================
# è¨­å®šå®šæ•°
# ==========================================
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_DIR = os.path.join(SCRIPT_DIR, "../LLaMA-Factory/data")
SAVE_FILE = os.path.join(OUTPUT_DIR, "math_synth_advanced.json")

# ãƒ¢ãƒ‡ãƒ«è¨­å®š (OpenRouter)
MODEL_MASS = os.getenv("MODEL_MASS", "deepseek/deepseek-chat")
MODEL_HARD = os.getenv("MODEL_HARD", "deepseek/deepseek-r1") 
MODEL_VERIFY = os.getenv("MODEL_VERIFY", "anthropic/claude-3.5-sonnet")

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š
PROMPT_MASS_SYSTEM = """ã‚ãªãŸã¯æ•°å­¦æ•™æã®ä½œæˆè€…ã§ã™ã€‚æŒ‡å®šã•ã‚ŒãŸãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã€æ˜ç¢ºã§æ•™è‚²çš„ãªæ•°å­¦ã®å•é¡Œã¨è©³ç´°ãªè§£ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
å¿…ãšJSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

PROMPT_HARD_SYSTEM = """ã‚ãªãŸã¯æ•°å­¦ã‚ªãƒªãƒ³ãƒ”ãƒƒã‚¯ãƒ¬ãƒ™ãƒ«ã®å•é¡Œä½œæˆè€…ã§ã™ã€‚
æ·±ãè¤‡é›‘ãªæ¨è«–ï¼ˆChain of Thoughtï¼‰ã‚’å¿…è¦ã¨ã™ã‚‹é›£å•ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’å«ã‚ã¦è§£ç­”ã‚’ä½œæˆã—ã€è«–ç†ã®é£›èºãŒãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
å¿…ãšJSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

PROMPT_VERIFY_SYSTEM = """ã‚ãªãŸã¯å³æ ¼ãªæ•°å­¦ã®æ¤œè¨¼è€…ã§ã™ã€‚
ä¸ãˆã‚‰ã‚ŒãŸæ•°å­¦ã®å•é¡Œã¨è§£ç­”ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€ä»¥ä¸‹ã®ç‚¹ã‚’æ¤œè¨¼ã—ã¦ãã ã•ã„ï¼š
1. æ•°å­¦çš„ã«æ­£ã—ã„ã‹
2. è¨ˆç®—ãƒŸã‚¹ã¯ãªã„ã‹
3. è«–ç†ã®é£›èºã¯ãªã„ã‹

å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆJSONï¼‰:
{
  "is_valid": true/false,
  "reason": "OK ã¾ãŸã¯ å…·ä½“çš„ãªã‚¨ãƒ©ãƒ¼å†…å®¹",
  "corrected_output": "å¿…è¦ã§ã‚ã‚Œã°ä¿®æ­£å¾Œã®è§£ç­”ã€ãªã‘ã‚Œã°null"
}
"""

PROMPT_USER_TEMPLATE = """ãƒˆãƒ”ãƒƒã‚¯: {topic}
ãƒ¬ãƒ™ãƒ«: {level}

ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’1ã¤ä½œæˆã—ã¦ãã ã•ã„:
{{
  "instruction": "å•é¡Œæ–‡...",
  "input": "",
  "output": "è§£ç­”..."
}}
"""

# ==========================================
# OpenRouter ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
# ==========================================
class OpenRouterClient:
    def __init__(self):
        api_key = os.getenv("OPENROUTER_API_KEY")
        if not api_key:
            raise ValueError("âŒ OPENROUTER_API_KEY ãŒ .env ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        
        self.client = OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
            default_headers={
                "HTTP-Referer": "https://github.com/Yuji181181/LLamaFactory-Unsloth-template", 
                "X-Title": "LLaMA-Factory Data Gen",
            }
        )

    def generate(self, model: str, system_prompt: str, user_prompt: str, json_mode: bool = True) -> Optional[Dict]:
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=0.7,
                response_format={"type": "json_object"} if json_mode else None
            )
            content = response.choices[0].message.content
            return json.loads(content)
        except Exception as e:
            logger.error(f"Generate Error ({model}): {e}")
            return None

# ==========================================
# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
# ==========================================
class DataPipeline:
    def __init__(self):
        self.api = OpenRouterClient()
        self.dataset = []

    def mass_produce(self, topics: List[str], count_per_topic: int):
        logger.info(f"ğŸš€ Mass Production Mode (Model: {MODEL_MASS})")
        for topic in topics:
            logger.info(f"Generating {count_per_topic} problems for: {topic}")
            for _ in tqdm(range(count_per_topic), desc=topic):
                data = self.api.generate(
                    model=MODEL_MASS,
                    system_prompt=PROMPT_MASS_SYSTEM,
                    user_prompt=PROMPT_USER_TEMPLATE.format(topic=topic, level="æ¨™æº–")
                )
                if data:
                    self.dataset.append(data)

    def hard_mode_produce(self, topics: List[str], count_per_topic: int):
        logger.info(f"ğŸ”¥ Hard Mode Production (Model: {MODEL_HARD})")
        for topic in topics:
            logger.info(f"Generating {count_per_topic} HARD problems for: {topic}")
            for _ in tqdm(range(count_per_topic), desc=topic + " (Hard)"):
                data = self.api.generate(
                    model=MODEL_HARD,
                    system_prompt=PROMPT_HARD_SYSTEM,
                    user_prompt=PROMPT_USER_TEMPLATE.format(topic=topic, level="é›£å•ãƒ»å¿œç”¨")
                )
                if data:
                    # R1ãªã©ã®æ€è€ƒãƒ¢ãƒ‡ãƒ«ç”¨ã«å¿œç­”ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’èª¿æ•´ã™ã‚‹å‡¦ç†ãŒå¿…è¦ãªå ´åˆã¯ã“ã“ã«è¿½åŠ 
                    self.dataset.append(data)

    def verify_and_clean(self):
        logger.info(f"âš–ï¸  Verification Mode (Model: {MODEL_VERIFY})")
        verified_dataset = []
        
        for data in tqdm(self.dataset, desc="Verifying"):
            verify_prompt = f"Problem: {data['instruction']}\nSolution: {data['output']}"
            result = self.api.generate(
                model=MODEL_VERIFY,
                system_prompt=PROMPT_VERIFY_SYSTEM,
                user_prompt=verify_prompt
            )
            
            if result and result.get("is_valid"):
                verified_dataset.append(data)
            elif result and result.get("corrected_output"):
                logger.info("ğŸ”§ Auto-corrected a problem.")
                data["output"] = result["corrected_output"]
                verified_dataset.append(data)
            else:
                logger.warning(f"âŒ Invalid data rejected: {result.get('reason') if result else 'Unknown error'}")

        self.dataset = verified_dataset

    def save(self):
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        with open(SAVE_FILE, "w", encoding="utf-8") as f:
            json.dump(self.dataset, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ Saved {len(self.dataset)} items to {SAVE_FILE}")

# ==========================================
# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# ==========================================
if __name__ == "__main__":
    pipeline = DataPipeline()
    
    # ãƒ†ã‚¹ãƒˆç”¨ãƒˆãƒ”ãƒƒã‚¯
    math_topics = ["å¾®åˆ†ç©åˆ†", "ç·šå½¢ä»£æ•°", "ç¢ºç‡è«–"]
    
    # 1. å¤§é‡ç”Ÿæˆ (DeepSeek-V3)
    pipeline.mass_produce(math_topics, count_per_topic=1)
    
    # 2. é›£å•ç”Ÿæˆ (DeepSeek-R1)
    pipeline.hard_mode_produce(["æ•°è«–", "ä½ç›¸å¹¾ä½•"], count_per_topic=1)
    
    # 3. æ¤œè¨¼ (Claude)
    pipeline.verify_and_clean()
    
    # ä¿å­˜
    pipeline.save()
